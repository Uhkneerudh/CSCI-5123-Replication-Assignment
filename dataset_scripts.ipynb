{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T12:55:50.837100Z",
     "start_time": "2023-04-27T12:55:50.744070Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enables auto-reload of files (%...function MUST BE WITHOUT SPACE!)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T13:31:36.008753Z",
     "start_time": "2023-04-27T13:31:35.907722Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import utils\n",
    "import score_util\n",
    "from  exploration import exploration_util\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as pltt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T12:58:19.278792Z",
     "start_time": "2023-04-27T12:58:19.174868Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define data locations\n",
    "data_location = '../../data/hummus_data/'\n",
    "graph_location = '../../data/food_kg/'\n",
    "additional_location = '../../data/hummus_data/' # if not present set recipe_tags=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-----------------------------------------------------------\n",
    "# Load & clean Dataset\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T13:02:48.311402Z",
     "start_time": "2023-04-27T12:58:28.298376Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import & clean data -- use k_user = 10, k_recipe=10\n",
    "recipes_df, reviews_df, users_df, recipes_dict, user_dict, food_locator_dict, food_com_dict, data = utils.load_and_clean_data(data_location, additional_location, k_user=10, k_recipe=10,\n",
    "                                                                                  add_recipe_columns=['food_kg_locator'],\n",
    "                                                                                  authorship_relations=6, recipe_tags=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sparsity Table (NEW CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalized Sparsity Generation Code (New Code)\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "sparsities = pd.DataFrame(index=['(10,10)','(100,2)','(2,50)','(2,100)'], columns=['#recipes','#users','#interac','sparsity'])\n",
    "\n",
    "k_values = [(10,10),(100,2),(2,50),(2,100)]\n",
    "\n",
    "for k in k_values:\n",
    "    # Import & clean data -- use k_user = 10, k_recipe=10\n",
    "    recipes_df, reviews_df, users_df, recipes_dict, user_dict, food_locator_dict, food_com_dict, data = utils.load_and_clean_data(data_location, additional_location, k_user=k[0], k_recipe=k[1],\n",
    "                                                                                    add_recipe_columns=['food_kg_locator'],\n",
    "                                                                                    authorship_relations=6, recipe_tags=True, debug=True)\n",
    "    \n",
    "    # Build a sparse matrix (user x recipe x ratings)\n",
    "    user_recipe_matrix = csr_matrix((reviews_df['rating'], (reviews_df['new_member_id'], reviews_df['new_recipe_id'])))\n",
    "\n",
    "    # All users and recipes\n",
    "    users = list(user_dict.keys())\n",
    "    recipes = list(recipes_dict.keys())\n",
    "\n",
    "    # Calculate the sparsity [in %]\n",
    "    A = user_recipe_matrix.toarray()\n",
    "    sparsity = 1.0 - (np.count_nonzero(A) / float(A.size) )\n",
    "\n",
    "    index = str(k)\n",
    "    sparsities.loc[index, '#recipes'] = len(recipes)\n",
    "    sparsities.loc[index, '#users'] = len(users)\n",
    "    sparsities.loc[index, '#interac'] = reviews_df.shape[0]\n",
    "    sparsities.loc[index, 'sparsity'] = (sparsity * 100) \n",
    "\n",
    "display(sparsities[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T13:15:02.771307Z",
     "start_time": "2023-04-27T13:08:27.951473Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute a dataframe consisting of nutrients normalized on 100g (takes ~15min), only necessary for the food score calculation\n",
    "normalized_ingredients = utils.normalize_ingredients(recipes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T13:17:51.690277Z",
     "start_time": "2023-04-27T13:16:33.366711Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all foodKG ingredient links\n",
    "ingredients_df, ingredients_dict, mapped_recipes = utils.load_ingredient_dict(recipes_df, graph_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T13:18:05.649025Z",
     "start_time": "2023-04-27T13:18:01.202147Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get food product label dict (from our data set)\n",
    "label_dict = utils.load_ingredient_tags(graph_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-----------------------------------------------------------\n",
    "# Calculate food scores\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T13:50:59.771048Z",
     "start_time": "2023-04-27T13:50:40.529143Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate food scores.\n",
    "recipes_df = score_util.calculate_food_scores(recipes_df, normalized_ingredients, score_names=['who', 'fsa', 'nutri'], normalize=True)\n",
    "recipes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T13:18:41.377152Z",
     "start_time": "2023-04-27T13:18:40.995585Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the score distribution histogram\n",
    "plt.hist(recipes_df['who_score'], bins=10)\n",
    "# Add labels and title\n",
    "plt.xlabel('Healthiness Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Healthiness Score Distribution')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T13:22:37.187175Z",
     "start_time": "2023-04-27T13:18:42.986219Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store preprocessed data\n",
    "recipes_df.to_csv(data_location + 'pp_recipes.csv')\n",
    "users_df.to_csv(data_location + 'pp_members.csv')\n",
    "reviews_df.to_csv(data_location + 'pp_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-----------------------------------------------------------\n",
    "# Explore data set\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T13:58:46.088732Z",
     "start_time": "2023-04-27T13:58:40.941368Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Merge data (again, because scores were not available before)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data_scores \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mmerge(recipes_df, reviews_df, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_recipe_id\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_recipe_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Copy data set for exploration and remove ingredient/duration/direction_size outliers (to compute nicer pictures)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m data_vis \u001b[38;5;241m=\u001b[39m exploration_util\u001b[38;5;241m.\u001b[39mremove_outliers(data_scores, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirection_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotalFat [g]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalories [cal]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotalCarbohydrate [g]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msugars [g]\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Merge data (again, because scores were not available before)\n",
    "data_scores = pd.merge(recipes_df, reviews_df, right_on='new_recipe_id', left_on='new_recipe_id')\n",
    "\n",
    "# Copy data set for exploration and remove ingredient/duration/direction_size outliers (to compute nicer pictures)\n",
    "data_vis = exploration_util.remove_outliers(data_scores, [\"duration\", \"direction_size\", \"totalFat [g]\", \"calories [cal]\", \"totalCarbohydrate [g]\", \"sugars [g]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T13:59:03.639021Z",
     "start_time": "2023-04-27T13:58:48.286733Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot duration & direction_size distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 8))\n",
    "sns.distplot(data_vis[\"duration\"], ax=ax[0]).set_xlim(0, 600)\n",
    "ax[1].xaxis.set_major_formatter(pltt.FuncFormatter(lambda x, _: int(x)))\n",
    "sns.distplot(data_vis[\"direction_size\"], ax=ax[1]).set_xlim(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T13:59:45.925746Z",
     "start_time": "2023-04-27T13:59:05.695023Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot ingredient distribution\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "fig, ax = plt.subplots(1, 4, figsize=(30, 10))\n",
    "ax[0].set_ylabel('Density', fontsize = 20)\n",
    "ax[0].set_xlabel('calories [cal]', fontsize = 20)\n",
    "sns.distplot(data_vis[\"calories [cal]\"].to_numpy(), ax=ax[0])\n",
    "ax[1].set_ylabel('', fontsize = 0)\n",
    "ax[1].set_xlabel('totalCarbohydrate [g]', fontsize = 20)\n",
    "sns.distplot(data_vis[\"totalCarbohydrate [g]\"].to_numpy(), ax=ax[1])\n",
    "ax[2].set_ylabel('', fontsize = 0)\n",
    "ax[2].set_xlabel('sugars [g]', fontsize = 20)\n",
    "sns.distplot(data_vis[\"sugars [g]\"].to_numpy(), ax=ax[2])\n",
    "ax[3].set_ylabel('', fontsize = 0)\n",
    "ax[3].set_xlabel('totalFat [g]', fontsize = 20)\n",
    "sns.distplot(data_vis[\"totalFat [g]\"].to_numpy(), ax=ax[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Format data for heatmap\n",
    "data_vis = data_vis.rename(columns={'servingSize [g]':'serSize', 'calories [cal]':'calories', 'caloriesFromFat [cal]':'calFromFat', 'totalFat [g]':'totalFat', 'cholesterol [mg]':'cholesterol', 'sodium [mg]':'sodium', 'dietaryFiber [g]':'dietaryFiber', 'sugars [g]':'sugars', 'protein [g]':'protein', 'saturatedFat [g]': 'satFat', 'totalCarbohydrate [g]': 'totalCarbs', 'direction_size': '#steps', 'ingredients_sizes': '#ingred', 'rating_y': 'rating'})\n",
    "data_vis.drop([\"new_recipe_id\"], axis=1, inplace=True)\n",
    "data_vis.drop([\"servingsPerRecipe\"], axis=1, inplace=True)\n",
    "data_vis.drop([\"new_member_id\"], axis=1, inplace=True)\n",
    "data_vis.drop([\"new_author_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vis = data_vis[[\"duration\",\"serSize\",\"calories\",\"calFromFat\",\"totalFat\",\"satFat\",\"cholesterol\",\"sodium\",\"totalCarbs\",\"dietaryFiber\",\"sugars\",\"protein\",\"#steps\",\"#ingred\",\"rating\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap of nutrients, direction_size, ingredient_size and ratings to show correlations\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(data_vis.corr(), annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-----------------------------------------------------------\n",
    "# Simple example recommender\n",
    "-----------------------------------------------------------\n",
    "## Implicit Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting implicit\n",
      "  Downloading implicit-0.7.2-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\aniru\\anaconda3\\envs\\vsc\\lib\\site-packages (from implicit) (1.26.4)\n",
      "Requirement already satisfied: scipy>=0.16 in c:\\users\\aniru\\anaconda3\\envs\\vsc\\lib\\site-packages (from implicit) (1.13.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aniru\\anaconda3\\envs\\vsc\\lib\\site-packages (from implicit) (4.66.2)\n",
      "Requirement already satisfied: threadpoolctl in c:\\users\\aniru\\anaconda3\\envs\\vsc\\lib\\site-packages (from implicit) (3.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aniru\\anaconda3\\envs\\vsc\\lib\\site-packages (from tqdm>=4.27->implicit) (0.4.6)\n",
      "Downloading implicit-0.7.2-cp310-cp310-win_amd64.whl (748 kB)\n",
      "   ---------------------------------------- 0.0/748.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 20.5/748.6 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 71.7/748.6 kB 787.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  747.5/748.6 kB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 748.6/748.6 kB 5.2 MB/s eta 0:00:00\n",
      "Installing collected packages: implicit\n",
      "Successfully installed implicit-0.7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from recommendation import implicit_util\n",
    "from scipy.sparse import csr_matrix\n",
    "import implicit.evaluation\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a sparse matrix (user x recipe x ratings)\n",
    "user_recipe_matrix = csr_matrix((reviews_df['rating'], (reviews_df['new_member_id'], reviews_df['new_recipe_id'])))\n",
    "\n",
    "# All users and recipes\n",
    "users = list(user_dict.keys())\n",
    "recipes = list(recipes_dict.keys())\n",
    "\n",
    "# Test/train split #Alternatively use implicit.evaluation.leave_k_out_split to force each user being in both sets\n",
    "train_matrix, test_matrix = implicit.evaluation.train_test_split(user_recipe_matrix.tocsr().tocoo())\n",
    "\n",
    "# Get users/recipes in the train set (or test set respectively)\n",
    "train_user, train_recipe = implicit_util.tuple_to_unique(train_matrix.tocsr().nonzero())\n",
    "test_user, test_recipe = implicit_util.tuple_to_unique(test_matrix.tocsr().nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.89351155631748\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sparsity [in %]\n",
    "A = user_recipe_matrix.toarray()\n",
    "sparsity = 1.0 - (np.count_nonzero(A) / float(A.size) )\n",
    "print(sparsity * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Executes all models, exception on Windows/Python3.10: nmslib_als, faiss_als\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import nmslib\n",
    "\n",
    "evaluation, recommendations, similar_items, similar_users = implicit_util.train_and_execute_all(train_matrix, test_matrix, train_user, train_recipe, ['bpr', 'faiss_als'], K=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Irec Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cachetools\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: cachetools\n",
      "Successfully installed cachetools-5.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cachetools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import irec_util\n",
    "from irec.environment.loader.full_data import FullData\n",
    "from irec.recommendation.agents.simple_agent import SimpleAgent\n",
    "from irec.recommendation.agents.action_selection_policies.egreedy import ASPEGreedy\n",
    "from irec.recommendation.agents.value_functions.e_greedy import EGreedy\n",
    "from irec.offline_experiments.evaluation_policies.fixed_interaction import FixedInteraction\n",
    "from irec.offline_experiments.metric_evaluators.user_cumulative_interaction import UserCumulativeInteraction\n",
    "from irec.recommendation.agents.action_selection_policies.greedy import ASPGreedy\n",
    "from irec.recommendation.agents.value_functions.best_rated import BestRated\n",
    "from irec.recommendation.agents.value_functions.entropy0 import Entropy0\n",
    "from irec.recommendation.agents.value_functions.ictr import ICTRTS\n",
    "from irec.recommendation.agents.value_functions.knn_bandit import kNNBandit\n",
    "from irec.recommendation.agents.value_functions.log_pop_ent import LogPopEnt\n",
    "from irec.recommendation.agents.value_functions.most_popular import MostPopular\n",
    "from irec.recommendation.agents.value_functions.pts import PTS\n",
    "from irec.recommendation.agents.value_functions.random import Random\n",
    "from irec.recommendation.agents.value_functions.thompson_sampling import ThompsonSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build interaction matrix\n",
    "pp_interactions = reviews_df[['new_member_id', 'new_recipe_id', 'rating', 'last_modified_date']]\n",
    "pp_interactions = pp_interactions.rename(columns={'new_member_id': 'user_id', 'new_recipe_id': 'item_id', 'last_modified_date': 'timestamp'})\n",
    "\n",
    "# Change timestamps and ratings to int\n",
    "pp_interactions['timestamp'] = pp_interactions.timestamp.values.astype(np.int64)\n",
    "pp_interactions['rating'] = pp_interactions['rating'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store preprocessed data\n",
    "output_path = './data/irec/'\n",
    "utils.ensure_dir(output_path)\n",
    "pp_interactions.to_csv(output_path + 'foodData.csv', sep=',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = {\n",
    "    'path': \"./data/irec/foodData.csv\",\n",
    "    'random_seed': 0,\n",
    "    'file_delimiter': \",\",\n",
    "    'skip_head': True\n",
    "}\n",
    "\n",
    "# Data Splitting\n",
    "splitting = {\n",
    "    'strategy': \"global\", # temporal, random, global, user_history\n",
    "    'train_size': 0.8,\n",
    "    'test_consumes': 5\n",
    "}\n",
    "\n",
    "# Loader\n",
    "loader = FullData(dataset, splitting)\n",
    "train_dataset, test_dataset, _, _ = loader.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define models\n",
    "## Evaluation Policy\n",
    "eval_policy = FixedInteraction(num_interactions=100, interaction_size=1, save_info=True)\n",
    "\n",
    "## Agents: value function & selection policy\n",
    "agents = []\n",
    "agents.append(SimpleAgent(Random(),ASPGreedy(),name=\"Random\"))\n",
    "agents.append(SimpleAgent(EGreedy(), ASPEGreedy(epsilon=0.1), name=\"EGreedy\"))\n",
    "agents.append(SimpleAgent(Entropy0(),ASPGreedy(),name=\"Entropy0\"))\n",
    "agents.append(SimpleAgent(LogPopEnt(),ASPGreedy(),name=\"LogPopEnt\"))\n",
    "agents.append(SimpleAgent(MostPopular(),ASPGreedy(),name=\"MostPopular\"))\n",
    "agents.append(SimpleAgent(BestRated(),ASPGreedy(),name=\"BestRated\"))\n",
    "agents.append(SimpleAgent(ThompsonSampling(alpha_0=1,beta_0=100),ASPGreedy(),name=\"ThompsonSampling\"))\n",
    "agents.append(SimpleAgent(ICTRTS(num_lat=2,num_particles=5),ASPGreedy(),name=\"ICTRTS\"))\n",
    "\n",
    "## Agents which take long\n",
    "agents.append(SimpleAgent(PTS(num_lat=20,num_particles=5,var=0.5,var_u=1.0,var_v=1.0),ASPGreedy(),name=\"PTS\"))\n",
    "agents.append(SimpleAgent(kNNBandit(alpha_0=1,beta_0=100,k=10),ASPGreedy(),name=\"kNNBandit\"))\n",
    "\n",
    "# Model runner\n",
    "runner = irec_util.Runner(train_dataset, test_dataset, eval_policy, repetitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running models\n",
    "results = runner.train_multiple_agents(agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "evaluator = UserCumulativeInteraction(\n",
    "    ground_truth_dataset=test_dataset,\n",
    "    num_interactions=20,\n",
    "    interaction_size=1,\n",
    "    interactions_to_evaluate=[10],\n",
    "    relevance_evaluator_threshold=3.99\n",
    ")\n",
    "\n",
    "evaluations = irec_util.calc_multiple_scores(evaluator, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning (NEW CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irec\n",
    "\n",
    "---\n",
    "\n",
    "**Random**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "## Evaluation Policy\n",
    "import random\n",
    "import time\n",
    "eval_policy = FixedInteraction(num_interactions=100, interaction_size=1, save_info=True)\n",
    "\n",
    "numEpochs = 20\n",
    "#Hyperparameter(s) to tune\n",
    "epsilonGrid = list(np.arange(0.01, 1.0, 0.01))\n",
    "\n",
    "evaluator = UserCumulativeInteraction(\n",
    "    ground_truth_dataset=test_dataset,\n",
    "    num_interactions=20,\n",
    "    interaction_size=1,\n",
    "    interactions_to_evaluate=[10],\n",
    "    relevance_evaluator_threshold=3.99\n",
    ")\n",
    "\n",
    "agents = []\n",
    "for i in range(numEpochs):\n",
    "## Agents: value function & selection policy\n",
    "    random.seed(time.perf_counter())\n",
    "    epsilon = random.randint(0, len(epsilonGrid)-1)\n",
    "    epsilon = round(epsilonGrid[epsilon], 2)\n",
    "    agents.append(SimpleAgent(EGreedy(), ASPEGreedy(epsilon=epsilon), name=str(epsilon)))\n",
    "\n",
    "# Model runner\n",
    "runner = irec_util.Runner(train_dataset, test_dataset, eval_policy, repetitions=1)\n",
    "results = runner.train_multiple_agents(agents)\n",
    "\n",
    "evaluations = irec_util.calc_multiple_scores(evaluator, results)\n",
    "e = evaluations[0][['precision','recall','p&r']]\n",
    "\n",
    "display(e.sort_values(by=['p&r']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ICTR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "## Evaluation Policy\n",
    "import random\n",
    "import time\n",
    "eval_policy = FixedInteraction(num_interactions=100, interaction_size=1, save_info=True)\n",
    "\n",
    "#section off dataset to 100,000 entries\n",
    "pp_interactions = pp_interactions[:100000]\n",
    "\n",
    "# Store preprocessed data\n",
    "output_path = './data/irec/'\n",
    "utils.ensure_dir(output_path)\n",
    "pp_interactions.to_csv(output_path + 'foodData.csv', sep=',', index = False)\n",
    "\n",
    "# Dataset\n",
    "dataset = {\n",
    "    'path': \"./data/irec/foodData.csv\",\n",
    "    'random_seed': 0,\n",
    "    'file_delimiter': \",\",\n",
    "    'skip_head': True\n",
    "}\n",
    "\n",
    "# Data Splitting\n",
    "splitting = {\n",
    "    'strategy': \"global\", # temporal, random, global, user_history\n",
    "    'train_size': 0.8,\n",
    "    'test_consumes': 5\n",
    "}\n",
    "\n",
    "# Loader\n",
    "loader = FullData(dataset, splitting)\n",
    "train_dataset, test_dataset, _, _ = loader.process()\n",
    "\n",
    "numEpochs = 20\n",
    "\n",
    "#Hyperparameter(s) to tune\n",
    "latGrid = list(np.arange(1, 10, 1))\n",
    "particlesGrid = list(np.arange(1, 20, 1))\n",
    "\n",
    "evaluator = UserCumulativeInteraction(\n",
    "    ground_truth_dataset=test_dataset,\n",
    "    num_interactions=20,\n",
    "    interaction_size=1,\n",
    "    interactions_to_evaluate=[10],\n",
    "    relevance_evaluator_threshold=3.99\n",
    ")\n",
    "\n",
    "agents = []\n",
    "for i in range(numEpochs):\n",
    "## Agents: value function & selection policy\n",
    "    random.seed(time.perf_counter())\n",
    "    lat = random.randint(0, len(latGrid)-1)\n",
    "    particle = random.randint(0, len(particlesGrid)-1)\n",
    "\n",
    "    lat = round(latGrid[lat], 2)\n",
    "    particle = round(particlesGrid[particle], 2)\n",
    "\n",
    "    agents.append(SimpleAgent(ICTRTS(num_lat=lat,num_particles=particle),ASPGreedy(),name=\"(\" + str(lat) + \", \" + str(particle) + \")\"))\n",
    "\n",
    "# Model runner\n",
    "runner = irec_util.Runner(train_dataset, test_dataset, eval_policy, repetitions=1)\n",
    "results = runner.train_multiple_agents(agents)\n",
    "\n",
    "evaluations = irec_util.calc_multiple_scores(evaluator, results)\n",
    "e = evaluations[0][['precision','recall','p&r']]\n",
    "\n",
    "display(e.sort_values(by=['p&r']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit\n",
    "\n",
    "---\n",
    "\n",
    "**ALS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sparse matrix (user x recipe x ratings)\n",
    "user_recipe_matrix = csr_matrix((reviews_df['rating'], (reviews_df['new_member_id'], reviews_df['new_recipe_id'])))\n",
    "\n",
    "# All users and recipes\n",
    "users = list(user_dict.keys())\n",
    "recipes = list(recipes_dict.keys())\n",
    "\n",
    "# Test/train split #Alternatively use implicit.evaluation.leave_k_out_split to force each user being in both sets\n",
    "train_matrix, test_matrix = implicit.evaluation.train_test_split(user_recipe_matrix.tocsr().tocoo())\n",
    "\n",
    "# Get users/recipes in the train set (or test set respectively)\n",
    "train_user, train_recipe = implicit_util.tuple_to_unique(train_matrix.tocsr().nonzero())\n",
    "test_user, test_recipe = implicit_util.tuple_to_unique(test_matrix.tocsr().nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executes all models, exception on Windows/Python3.10: nmslib_als, faiss_als\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import nmslib\n",
    "import random\n",
    "import time\n",
    "\n",
    "evaluations = pd.DataFrame(columns=['parameters', 'precision', 'map', 'ndcg', 'total'])\n",
    "\n",
    "numEpochs = 20\n",
    "factorGrid = list(np.arange(1, 20, 1))\n",
    "regularizationGrid = list(np.arange(0.01, 1.0, 0.01))\n",
    "\n",
    "for i in range(numEpochs):\n",
    "    random.seed(time.perf_counter())\n",
    "    factors = random.randint(0, len(factorGrid)-1)\n",
    "    regularization = random.randint(0, len(regularizationGrid)-1)\n",
    "\n",
    "    factors = round(factorGrid[factors], 2)\n",
    "    regularization = round(regularizationGrid[regularization], 2)\n",
    "\n",
    "    evaluation, recommendations, similar_items, similar_users = implicit_util.train_and_execute_all(train_matrix, test_matrix, train_user, train_recipe, ['bpr', 'nmslib_als', 'lmf', 'bm25', 'cosine', 'tfidf', 'ii', 'annoy_als'], factors=factors, regularization = regularization, K=10)\n",
    "    parameters = '(' + str(factors) + ', ' + str(regularization) + ')'\n",
    "    precision = evaluation['p@10'].values[0]\n",
    "    map = evaluation['map@10'].values[0]\n",
    "    ndcg = evaluation['ndcg@10'].values[0]\n",
    "    total = precision + map + ndcg\n",
    "    evaluations.loc[evaluations.shape[0]] = [parameters, precision, map, ndcg, total]\n",
    "\n",
    "display(evaluations.sort_values(by=['total'],ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LMF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executes all models, exception on Windows/Python3.10: nmslib_als, faiss_als\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import nmslib\n",
    "import random\n",
    "import time\n",
    "\n",
    "evaluations = pd.DataFrame(columns=['parameters', 'precision', 'map', 'ndcg', 'total'])\n",
    "\n",
    "numEpochs = 20\n",
    "factorGrid = list(np.arange(20, 40, 1))\n",
    "regularizationGrid = list(np.arange(0.5, 2.5, 0.1))\n",
    "\n",
    "for i in range(numEpochs):\n",
    "    random.seed(time.perf_counter())\n",
    "    factors = random.randint(0, len(factorGrid)-1)\n",
    "    regularization = random.randint(0, len(regularizationGrid)-1)\n",
    "\n",
    "    factors = round(factorGrid[factors], 2)\n",
    "    regularization = round(regularizationGrid[regularization], 2)\n",
    "\n",
    "    evaluation, recommendations, similar_items, similar_users = implicit_util.train_and_execute_all(train_matrix, test_matrix, train_user, train_recipe, ['bpr', 'nmslib_als', 'als', 'bm25', 'cosine', 'tfidf', 'ii', 'annoy_als'], factors=factors, regularization = regularization, K=10)\n",
    "    parameters = '(' + str(factors) + ', ' + str(regularization) + ')'\n",
    "    precision = evaluation['p@10'].values[0]\n",
    "    map = evaluation['map@10'].values[0]\n",
    "    ndcg = evaluation['ndcg@10'].values[0]\n",
    "    total = precision + map + ndcg\n",
    "    evaluations.loc[evaluations.shape[0]] = [parameters, precision, map, ndcg, total]\n",
    "\n",
    "display(evaluations.sort_values(by=['total'],ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bm25**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executes all models, exception on Windows/Python3.10: nmslib_als, faiss_als\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import nmslib\n",
    "import random\n",
    "import time\n",
    "\n",
    "evaluations = pd.DataFrame(columns=['parameters', 'precision', 'map', 'ndcg', 'total'])\n",
    "\n",
    "numEpochs = 10\n",
    "kGrid = list(np.arange(50, 150, 1))\n",
    "bGrid = list(np.arange(0.01, 1, 0.01))\n",
    "\n",
    "for i in range(numEpochs):\n",
    "    random.seed(time.perf_counter())\n",
    "    K1 = random.randint(0, len(kGrid)-1)\n",
    "    B = random.randint(0, len(bGrid)-1)\n",
    "\n",
    "    K1 = round(kGrid[K1], 2)\n",
    "    B = round(bGrid[B], 2)\n",
    "\n",
    "    evaluation, recommendations, similar_items, similar_users = implicit_util.train_and_execute_all(train_matrix, test_matrix, train_user, train_recipe, ['bpr', 'nmslib_als', 'als', 'lmf', 'cosine', 'tfidf', 'ii', 'annoy_als'], K1=K1, B=B, K=10)\n",
    "    parameters = '(' + str(K1) + ', ' + str(B) + ')'\n",
    "    precision = evaluation['p@10'].values[0]\n",
    "    map = evaluation['map@10'].values[0]\n",
    "    ndcg = evaluation['ndcg@10'].values[0]\n",
    "    total = precision + map + ndcg\n",
    "    evaluations.loc[evaluations.shape[0]] = [parameters, precision, map, ndcg, total]\n",
    "\n",
    "display(evaluations.sort_values(by=['total'],ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## MS Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cornac_util\n",
    "import torch\n",
    "import cornac\n",
    "from recommenders.utils.constants import SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build feature matrix\n",
    "pp_interactions = reviews_df[['new_member_id', 'new_recipe_id', 'rating']]\n",
    "pp_interactions = pp_interactions.rename(columns={'new_member_id': 'userID', 'new_recipe_id': 'itemID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store preprocessed data\n",
    "output_path = './data/cornac/'\n",
    "utils.ensure_dir(output_path)\n",
    "pp_interactions.to_csv(output_path + 'foodData.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read & split data\n",
    "pp_interactions, train, test, train_set = cornac_util.load_and_split()\n",
    "print(pp_interactions.shape)\n",
    "pp_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set global model parameters\n",
    "## top k items to recommend\n",
    "TOP_K = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BPR\n",
    "bpr = cornac.models.BPR(\n",
    "    k=10,  #200\n",
    "    max_iter= 10,  #100,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.001,\n",
    "    verbose=True,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BiVAE\n",
    "bivae = cornac.models.BiVAECF(\n",
    "    k=50,\n",
    "    encoder_structure=[100],\n",
    "    act_fn=\"tanh\",\n",
    "    likelihood=\"pois\",\n",
    "    n_epochs=10, #500\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    seed=SEED,\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "models = [bivae, bpr]\n",
    "cornac_util.train_multiple(models, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "model_predictions = cornac_util.predict_multiple(models, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "evaluation = cornac_util.calc_scores(test, model_predictions, TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "display(evaluation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.ensure_dir('./data/irec/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Models (Surprise, NEW CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: surprise in c:\\users\\aniru\\anaconda3\\envs\\vsc\\lib\\site-packages (0.1)\n",
      "Requirement already satisfied: scikit-surprise in c:\\users\\aniru\\anaconda3\\envs\\vsc\\lib\\site-packages (from surprise) (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\aniru\\anaconda3\\envs\\vsc\\lib\\site-packages (from scikit-surprise->surprise) (1.4.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\aniru\\anaconda3\\envs\\vsc\\lib\\site-packages (from scikit-surprise->surprise) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\aniru\\anaconda3\\envs\\vsc\\lib\\site-packages (from scikit-surprise->surprise) (1.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601887, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build feature matrix\n",
    "pp_interactions = reviews_df[['new_member_id', 'new_recipe_id', 'rating']]\n",
    "pp_interactions = pp_interactions.rename(columns={'new_member_id': 'userID', 'new_recipe_id': 'itemID'})\n",
    "pp_interactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://surprise.readthedocs.io/en/stable/FAQ.html#how-to-compute-precision-k-and-recall-k\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "def ndcg(surprise_predictions, M=18316, N=30859, k=10):\n",
    "    \"\"\"\n",
    "    Calculates the ndcg (normalized discounted cumulative gain) from surprise predictions, you could use sklearn.metrics.ndcg_score and scipy.sparse\n",
    "\n",
    "    Parameters:\n",
    "    surprise_predictions (List of surprise.prediction_algorithms.predictions.Prediction): list of predictions,\n",
    "    see https://surprise.readthedocs.io/en/stable/predictions_module.html?highlight=prediction#surprise.prediction_algorithms.predictions.Prediction\n",
    "    M: number of users\n",
    "    N: number of movies\n",
    "    k (positive integer): Only consider the highest k scores (items) in each user's recommendation list.\n",
    "\n",
    "    Returns:\n",
    "    float in [0., 1.]: The averaged NDCG scores over all users' recommendation lists.\n",
    "\n",
    "    \"\"\"\n",
    "    # Task 3 to do start: return the ndcg score, with the help of sklearn ndcg_score package\n",
    "    # hint: build 2 M x N matrices, each row representing the predicted/true ratings for one user -- and input that 2 matrices into ndcg_score directly.\n",
    "\n",
    "    predicted_ratings = np.zeros((M, N))\n",
    "    true_ratings = np.zeros((M,N))\n",
    "\n",
    "    #Populate Matrices\n",
    "    for prediction in surprise_predictions:\n",
    "      user_id = int(prediction.uid)\n",
    "      movie_id = int(prediction.iid)\n",
    "      pred_rating = prediction.est\n",
    "      true_rating = prediction.r_ui\n",
    "\n",
    "      predicted_ratings[user_id-1][movie_id-1] = pred_rating\n",
    "      true_ratings[user_id-1][movie_id-1] = true_rating    \n",
    "\n",
    "    #array to track ndcg scores \n",
    "    ndcg_scores = []\n",
    "    for user_id in range(M):\n",
    "      #calculate ndcg score for given user (must convert ratings to numpy array to be fed into ndcg_score)\n",
    "      pred = np.asarray([predicted_ratings[user_id]])\n",
    "      truth = np.asarray([true_ratings[user_id]])\n",
    "      ndcg_scores.append(ndcg_score(truth, pred, k=k))\n",
    "      \n",
    "    # Calculate average NDCG score across all users\n",
    "    avg_ndcg_score = sum(ndcg_scores) / M\n",
    "\n",
    "    return avg_ndcg_score\n",
    "\n",
    "    # Task 3 to do end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, BaselineOnly, KNNBasic, NMF, accuracy, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "metric_report = pd.DataFrame(index=['Bias', 'UserUser','ItemItem','NMF', 'SVD'], columns=['precision@10','recall@10','ndcg@10'])\n",
    "\n",
    "# use the built-in funk svd at one parameter\n",
    "# Task 6 to do start: fill in metric_report\n",
    "\n",
    "algorithms = {\n",
    "    'Bias': BaselineOnly(),\n",
    "    'UserUser': KNNBasic(sim_options={'user_based': True}),\n",
    "    'ItemItem': KNNBasic(sim_options={'user_based': False}),\n",
    "    'NMF': NMF(),\n",
    "    'SVD': SVD(n_factors=10, n_epochs=20),\n",
    "}\n",
    "\n",
    "reader = Reader(rating_scale=(1, 6))\n",
    "for algorithm_name, algorithm in algorithms.items():\n",
    "\n",
    "    data = Dataset.load_from_df(pp_interactions, reader)\n",
    "\n",
    "    train, test = train_test_split(data,test_size=0.2)\n",
    "\n",
    "    algorithm.fit(train)\n",
    "    predictions = algorithm.test(test)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=10)\n",
    "    ndcg_value = ndcg(predictions)\n",
    "\n",
    "    # Calculate Metrics and Update metric_report\n",
    "    metric_report.loc[algorithm_name, 'precision@10'] = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    metric_report.loc[algorithm_name, 'recall@10'] = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    metric_report.loc[algorithm_name, 'ndcg@10'] = ndcg_value\n",
    "\n",
    "display(metric_report)\n",
    "# Task 6 to do end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vsc)",
   "language": "python",
   "name": "vsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
